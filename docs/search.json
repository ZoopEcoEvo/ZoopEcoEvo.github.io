[
  {
    "objectID": "00_index.html",
    "href": "00_index.html",
    "title": "Intro to the Work Flow",
    "section": "",
    "text": "In general, operating in academia means most projects will be geared towards eventual publication. Our goal is not just to share results, however, but to help people understand the analyses we did. All publications from our group will therefore include some way of accessing the associated data and scripts. However, this gets us only partway towards our goal of replicable and understandable analyses. Project materials need to be well organized and thoroughly documented (not just “accessible”) in order for other people to follow what was done. This will also help other lab members, if at some point someone wants to follow up on your project, expand the scope of data you collected, or apply similar analyses to their data sets. This goal of making your work accessible, replicable, and understandable shapes the workflow I will ask you to use.\n\nProject Folders\nThis workflow is organized around a “project folder” - a collection of all materials (background information, data, scripts, outputs, etc.) used for a project, contained within a single directory. This concept is described in these slides and here. I’ve set up a template project folder that we’ll generally use to get your work started. A map of this project folder is included below to give you some idea of the framework the workflow described on this page works within. How the project folder functions during Project Development is expanded in that section.\n\nAs you can probably already tell, the workflow also relies on Git and GitHub to version control your work, to collaborate and review code, and to share data and results. Project folders and GitHub integrate nicely, and we’ll use a specific workflow while developing projects and writing code based on “GitHub flow”.\n\nSee here for the GitHub description of this approach and here for more details on how it’s applied in a research lab setting.\n\n\n\nInitial Setup\nThis workflow tends to “just work” once everything is set up and you get familiar with it. However, there’s a fair amount of stuff to work out before you get to that point. Luckily, these are generally one-time investments (per machine, that is).\n\nYou should already be added to the GitHub organization as a member, but if not, remind me.\nInstall R and RStudio.\nMake sure you have git installed on your local machine by this point. There are some helpful instructions here if needed.\nMake sure you’ve got the connections between Git, GitHub, and RStudio worked out. See here for a nice walk-through. We will generally use HTTPS when working with GitHub in RStudio.\nIt’s a good idea to have a basic familiarity with the terminology of Git and GitHub before you really dive in. For example, you should know what “pull”, “commit”, “push”, and “branch” mean in the context of a GitHub repo. See:\n\nHere for the GitHub documentation, including separate pages for commits, pulling, and pushing. Note that while these pages assume you’ll be working with Git in the command line, rather than RStudio, the general descriptions still apply.\nHere for a very in-depth written intro.\nHere for a quick video introduction (starts at ~1:20 where the description of Git starts).\nHere for a nice visual representation of some of the basis Git/GitHub actions.\nHere for a walk-through of a fairly similar Git/GitHub/RStudio workflow. There’s some differences (or things you won’t have to worry about because I will take care of it for you when we set your project up), but the sections on ‘Creating an RStudio project from version control’ and ‘Working with Git from within RStudio’ are definitely relevant.\n\n\n\n\nGeneral Note\nIn addition to the data and code you work on, you can make valuable contributions to the development of the lab as a whole by writing up the “invisible” work that you did - what did it take to get to a point where you could begin to engage with the project work flow? Did you find any useful resources that helped you? Are there aspects that were confusing or poorly explained to you? Were there obstacles that could be removed? After working with these systems for a while, it’s difficult to accurately remember what it’s really like to be faced with these challenges for the first time - your insights here can help design as smooth an on-boarding experience as possible!\n\nSee here for the initial inspiration for this."
  },
  {
    "objectID": "03b_pull_requests.html",
    "href": "03b_pull_requests.html",
    "title": "Pull Requests",
    "section": "",
    "text": "Before you know it, you’ll have made some major progress on your project. Let’s say you’ve finished running some analyses and made a couple nice figures, backing up the changes made with Git/GitHub along the way. You’re ready to get some feedback on your work, and include the progress you’ve made in the “official” version of the project on the main branch. Remember, however, that this main branch will generally be “read only”. The process of getting feedback and adding new material to the main branch are interlinked in our workflow, mediated by something called a “pull request” (PR) - a request you make for the main branch to pull in your work. When successfully completed, this will merge your working branch into the main branch. Because this will alter the “official” version of the project, the code involved in a PR has to be checked over by someone, which creates the opportunity to give feedback / suggestions.\nBefore you initiate the PR, make sure your local branch is up to date by doing one last pull-push cycle from RStudio. This will make sure that:\n\nyour local code reflects the most up-to-date version on the repo to catch any last merge conflicts, and\nthat your branch on the virtual repo reflects all the changes you’ve made.\n\nOnce you’re sure everything is up to date, initiate the PR by clicking “New Pull Request” on GitHub. This should be visible when viewing the active branches tab. Doing this manually on GitHub rather than through RStudio is the easiest way to start the PR.\nThis will then start the Code Review process. I will automatically be notified, but you can also mention any other lab member in the PR that you’d like to look over your code (just be mindful about how much time you’re asking from other lab members). During code review, we will typically start by looking over the output (either the HTML or the github doc/markdown in the Reports directory) since this should include both code and graphical outputs. Be sure to specify if there are additional files you’re looking for feedback on. Depending on time available and complexity of the code, we may also review the rest of the project folder (other scripts, raw data, output data, etc.). We’ll make comments and suggestions for changes, or accept the PR and merge the development branch into the main branch. Once merged, this now stands as the new official version, and you can start a new branch to work on the next project development goal.\nThe Pull Request is central to the workflow, and as you’ll see is used across all phases of project development. This process is iterative, by design! And while it might take some time and effort to get the code worked out, this will hopefully help to develop your coding skills and catch potential mistakes before they get embedded in papers and presentations. Equally important, it distributes the responsibility for checking the code across the group - If we all benefit from your work, we should all share some of this burden.\n\nIssues or Pull Requests?\nLet’s say you’re working on a project and have a small issue/solution/chunk of code you want some feedback on. If there’s no major progress to report yet, it might not be worth it to go through the whole process of starting a PR. There are two other ways to get feedback:\n\nThe simplest: bring it up during our regularly scheduled meeting. If you think it’ll take up a bit of time, you can always ask to set up a separate meeting as well.\nYou can also raise an issue on GitHub. Along the top bar of the repo, you’ll see an “Issues” tab. By clicking “New Issue” you’ll be able to ask questions, bring attention to problems, etc. You can also tag people if there’s someone specific you want to ask. Be sure to include exactly how to find the relevant code (a line number and file name, for example). Avoid the temptation to send code or code chunks via email - keep it on GitHub."
  },
  {
    "objectID": "03a_data_analysis.html",
    "href": "03a_data_analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "The heavy lifting is done via (a series of) data processing scripts, housed within the Scripts directory. This should include any major carpentry (reshaping, organizing, filtering, etc.), along with any intensive statistical analyses. The output of both should be saved to the Outputs directory. THE RAW DATA SHOULD NEVER BE OVERWRITTEN! The goal here is to retain the raw data while also reducing the run time / computation power required for data visualization and manuscript prep. As you’ll see, both phases are highly iterative (you’ll usually knit the same document many times before it’s ready to be sent out), and the last thing you’ll want is for that process to be bogged down by computationally intensive processes or analyses.\nIn order to balance the need to retain the raw data and also reduce run times, data processing scripts read in raw data and save processed data, which can be used downstream for visualizations and other purposes.\n\nSplitting Scripts\nThe same principle can be applied to your data processing scripts; if you’ve got several different analyses that go into processing your data, and one is really long and computationally intensive, you can split the different processing steps into separate scripts. Once you modify the controller script, you can pick and choose when each processing step should be run."
  },
  {
    "objectID": "03_project_dev.html",
    "href": "03_project_dev.html",
    "title": "Project Development",
    "section": "",
    "text": "After the planning stages, work on the project will generally fall into three main categories (there can be lots of variation between projects though): data analysis, project reports, and manuscript prep. These three classes of project development will pull from different components of the project folder, as illustrated below.\n\nDespite the differences, though, each class will depend on similar skill sets and development follows a similar workflow. Further, when using the project folder approach we’ve already described, all three components of project development are run through / woven together by the controller script. The general outline of what this script looks like is also included above."
  },
  {
    "objectID": "03_project_dev.html#key-settings",
    "href": "03_project_dev.html#key-settings",
    "title": "Project Development",
    "section": "Key Settings",
    "text": "Key Settings\nAs the goal of an R Project Folder is to help make analyses reproducible, there are a couple default settings in R that you will want to change. For those interested, the changes to these settings are one of the things stored in the .Rproj file.\n\nNever save .Rdata when closing your session. Open up your Preferences > General, and make sure “Restore .Rdata into workspace at startup” is not checked. This will ensure that variables and other objects from your Environment are not carried over from one session to the next. Also ensure that “Save workspace to .Rdata on exit” is set to “Never”.\nIn Preferences > R Markdown, make sure that “Evaluate chunks in directory” is set to “Project”. This will make it easier to navigate through the component folders when setting file paths.\nIn the drop-down menu next to the “Knit” button, ensure that “Knit directory” is set to “Project”."
  },
  {
    "objectID": "03d_manuscripts.html",
    "href": "03d_manuscripts.html",
    "title": "Manuscript Prep",
    "section": "",
    "text": "Another Benefit of the Project Folder\nProject folders are also set up to help integrate figures, statistical results, etc. into the manuscript writing process. By keeping all relevant files in the same project folder, and using the controller script to weave different components together, it should be easier to write (and then update) a manuscript draft with the correct results. For example, smart usage of inline code will save you from having to manually update statistical results in tables and text throughout the draft when changes are made (saving time and helping to avoid confusion down the road!). And by using the files in the Output directory to place figures, the manuscript should always be pulling from up-to-date visualizations.\nThere should be a directory entitled “Manuscript” within each project folder, which includes a .Rmd file (the manuscript file), along with .sty and .bib files (containing formatting details and the bibliography, respectively). By default, these will produce a PDF formated in the Arxiv style and a word document when the .Rmd file is knit. This is different from the report .Rmd file (which by default outputs an HTML and special markdown file) - the main difference here is that code chunks will not appear in the PDF or word documents, while they do in the HTML and markdown outputs. As with the analysis scripts and report markdown file, the manuscript markdown file should be run through the controller script.\n\n\nGetting Started\nBefore you start writing the first draft, rename the .Rmd file name to reflect the manuscript. Typically we will format the file name as it would be referred to in an in-text citation (ex - LeadAuthor_et_al_2022, Author1_and_Author2_2022, etc.). If you’ve got multiple files with the same name (meaning multiple papers per project - nice job!), you can append letters to the name (LeadAuthor_et_al_2022a, LeadAuthor_et_al_2022b, etc.) or add a little context (LeadAuthor_et_al_2022_plasticity, LeadAuthor_et_al_2022_latitudinal, etc.).\nKeep the use of code within the manuscript .Rmd file to a minimum. All major data analysis and figure generation should be performed in other scripts within the project folder. When placing figures, for example, it will be substantially faster to use the image files output by the Report.Rmd file than to create the figures from code. Given the number of times you’ll likely knit a manuscript during the writing and revision process, this saved time adds up. It is therefore in your best interest to ensure that the report.Rmd file (or whichever script is outputting the image files) reflects publication-ready figures whenever possible.\nThe use of a reference manager is strongly recommended. RStudio has a built in citation function, but the ability to store and organize references outside of the project is also useful. Zotero is an open source reference manager that integrates nicely with RStudio and markdown documents. This is the program I use/recommend. There’s a useful write up of how citations work in .Rmd files here. Note, you’ll need to be using RStudio version 1.4 or later to access these features.\n\n\nGetting Feedback\nThe workflow for writing the manuscript draft will be similar to that used during the Project Development phase, at least while putting together the initial versions. Changes are made on a working branch, which are reviewed via Pull Request. If comments are required from co-authors who do not use GitHub, it is the responsibility of the lead author to ensure they are sent an up-to-date word document (or Google Doc version) and that their comments are properly integrated into the GitHub repo.\nAs you approach the submission stage (the manuscript draft is completed and just being polished now), it’s time to start thinking about how the data and scripts will be shared with people outside our group. One easy approach is to archive the GitHub repo with Zenodo. This allows you designate a Version of Record, which has an associated DOI. See here for more details.\nOnce the new repo DOI is included in a Data Availability statement in the manuscript, it’s ready for submission. The .Rmd file is knit and the resulting PDF and word documents are prepared for submission to a preprint server and journal, respectively.\nDuring peer review, a manuscript will be read and evaluated by several reviewers who will give (varying amounts of) feedback. Once you get these reviews, you can start a new working branch for a second version of the manuscript and start making the recommended changes. Here again the benefits of using a project folder are evident - depending on the feedback you might have to make changes to the analyses or figures, which will be automatically incorporated into the manuscript (at least where file paths and inline code are used)."
  },
  {
    "objectID": "02a_Tutorial_1.html",
    "href": "02a_Tutorial_1.html",
    "title": "Part 1 - R Basics",
    "section": "",
    "text": "This first section will introduce the basic elements and components of R Studio, as well as begin to shape your understanding of R syntax and how it can be used to examine and manipulate data.\nR is a coding language. Technically you can do everything you need in the terminal window. However, this is not only challenging, but can limit the scope of what’s possible with R. R Studio is a graphical user interface (GUI) that allows you to simultaneously write R scripts, keep track of variables and data objects you’ve created, and visualize your data. It also has a built in support structure, complete with help pages and tutorials.\nR can be downloaded here.\nR Studio can be downloaded here. Download the open-source Desktop version.\nYou will need both installed before continuing with these activities. This tutorial will provide both examples of lines of code and their output, as well as prompts for you to write code yourself. It will be helpful to “code along” and replicate all of the provided examples as we go.\n\n\n\nAn R Studio window has four major components. The Console window, in the lower left quadrant, is used for the direct input of commands. This is a direct link to your computer and R functionality. It can be used for testing code or running quick calculations. It’s also useful for examining the contents of data structures. It’s not very efficient for creating and storing variables and commands though. For this, we will rely on a document called a ‘script’ (shown here in the top left quadrant) Make sure you have a new script open as we move through these exercises (you can create a blank script using the ‘new file’ icon in the top left corner of R Studio). The ability to open, edit, and run R scripts in R Studio is one of the biggest advantages of using the GUI. These scripts are used to write and record commands, providing a written record of everything you do to get from raw data to final product. This in turn makes your analyses easier to repeat. Functional and logical R scripts are an important component of many open science ventures.\nThe R Studio environment also provides easy access to other important details. The Files, Plots, Packages, and the Help windows share a space in R Studio (lower right quadrant). The Files window shows you the files in your working directory, and can be used to access data. The Plots window is where graphical outputs will show up. The range of things you can do with R is greatly expanded by packages (essentially, your R toolbox). These contain functions for diverse applications, and can be used to customize your R experience. The Packages window shows which packages have been installed, and which are currently active. This area can also be used to find and install new packages. We will install several important packages later on. The Help window is relatively self-explanatory. Functions and packages come with documentation, which can be accessed via the Help window. You can use the search bar in that window, or simply type ‘?function_name()’ into the console.\nThe final area of an R Studio workspace contains the Environment and History windows. Your Environment window will show you all of the variables and data objects you’ve generated. The History window records all of your actions. This can be useful if you want to trace your steps backwards through an analysis to confirm what you did or identify an issue.\nThe arrangement of these windows and general appearance of the GUI is customizable in Preferences/Settings.\n\n\n\nYour interaction with R is based on commands. Remember, R is a computer language. Despite all efforts to make it as user friendly as possible, it can neither fully anticipate your needs nor infer your intentions. Correct syntax and usage of commands is therefore of utmost importance (and where 99% of the frustration with coding will lie).\nR recognizes three types of commands. Elementary commands are fairly intuitive. These include things like basic mathematical expressions and variable assignment. Remember, commands can be input into either the Console or the R script. Commands in the Console will run automatically after hitting the ‘enter’/‘return’ key. There are several ways to run a command contained in a script. The “Run” button in the top right section of the script window will run the current line or all lines selected. The “Source” button will run the entire script. There are also system-specific keyboard shortcuts to run a line of code; ‘command + return’ on a Mac, for example. It will be helpful to find the shortcut for whichever system you’re using. After running code in the script window, results will show up in the console window. Practice entering and running the following elementary commands in both windows.\n\n1 + 1  \n## [1] 2\n\n\n2 - 1  \n## [1] 1\n\n\n12 * 2  \n## [1] 24\n\nNote that input of an expression without assignment (using an ‘=’) prints the result in the console window, but does not store the value (you can see what you’ve stored in the Environment window). In order to store values, you need to assign them to a variable. A variable can be single characters (ex - A, B, X, Y) or strings of characters (ex - key_values, LD50, data.set). Variable names cannot, however, start with a number. We’ll go over assignment to variables in greater detail in a later section, but for now try assigning a few variables.\n\nvariable = 1\n\nprint(variable)\n## [1] 1\n\n\nvariable = 3 + 5\n\nprint(variable)\n## [1] 8\n\nNote: In the above examples, we use print() to see the content of a variable. You can also see what a variable just by running the variable by itself, but it’s a good idea to use a command like print() so you know that you didn’t lose any code from that line.\n\nprint(variable)\n## [1] 8\n\n#Prints the same thing\nvariable \n## [1] 8\n\nThe real power of R is in the use of function commands. Functions tell R to do a series of operations, which can include other mathematical functions (find the mean, the standard deviation, etc.), or more complex actions (estimate linear regressions, make plots, etc.). These functions follow a basic syntax of FUNCTION(parameters and values).\n\nmean(c(1, 14, 10, 6, 2, 2, 9))\n## [1] 6.285714\n\nRemember, R cannot ‘infer’ the end of incomplete commands. R can tell you when it thinks a command is incomplete though; incorrect syntax will return an error or will return a line that starts with + rather than > in the console window. For example, try running the previous line of code without the last parenthesis. You can ‘complete’ the line of code by entering the missing closing parenthesis in the Console window. ‘Control + c’ can also be used to abort an incomplete command.\n\n\n\nAs we move through these workshop sessions, there is one key thing to keep in mind. This what we’ll call the Golden Rule of R: Be kind to yourself. Developing good habits early on is critical!\nObviously the longer you use R, the more functions you’ll come across (the collector’s curve will never saturate). Keep track of particularly important or useful commands. It can be helpful to maintain a separate document with these commands and what they do (think of this as a personalized diction-R-y). This is something you can refer back to often, rather than needing to search through large volumes of old scripts for a specific line of code. This can be tedious, and depends a lot on personal preference. You might want to keep track of just the functions that give you trouble, or that you can’t ever seem to remember. You will develop a sense of what is most useful for you as you spend more time coding.\nA comprehensive repository is less useful for variables, as even relatively simple processes in R can generate dozens of them. Keeping track of all of them in one place would be tiresome and inefficient. You still need to keep track of what you’re creating (and why) though. There are two good habits that will help with this. First, develop a standard naming structure for your variables. Names should be both concise and descriptive. For instance, imagine you’re creating a model describing the relationship between X and Y, and want to store the model as a variable. Assigning it to a variable called ‘model_describing_the_relationship_between_X_and_Y’ is fairly descriptive, but way too long. A variable called ‘model’ is short, but not descriptive. Variables like ‘X_Y.model’ are typically a good compromise. However you decide to name variables, be consistent for different types of objects across your scripts. If you use ‘X_Y.model’ in one script, use ‘A_B.model’ rather than ‘AB_model’ for the next model. These stylistic conventions are usually based on personal preference, and should be something you include in your dictionary.\nIt’s also important that variable names be relatively unique: You want to avoid using the same variable name across many different scripts. For example, using a variable called “data” to store your data in all of your scripts is an easy way to get your analyses tangled up, especially if you’re jumping between different R scripts in the same session. Assigned variables are not stored in your script. So, if you assign something to ‘X_data’ in one script, it will carry that over into any other script you open during that session. Likewise, if you re-assign ‘X_data’ in the second script and then go back to the first script, ‘X_data’ won’t change back.\nFinally, remember that R is case sensitive; ‘Variable’ is not the same thing as ‘variable’ in R. Despite the scientific convention (e.g. - The ‘t’ vs. ‘T’ variables for time and temperature), it’s usually not a good idea to differentiate variables based soley on capitalization. Remember that the goal is to be both concise and descriptive - name your variables ‘time’ and ‘temp’, not ‘t’ and ‘T’.\nThe second (and more important) helpful habit is to comment on everything! While a separate document for important code/variables can be useful for general R purposes, it will not help you remember the purpose, organization, or workflow of an individual script. For this, we rely on in-script annotation. Characters following a # are not executed by R. These are called comments, and are absolutely essential for long-term success in R. You should be able to open up an R script and know exactly what every line is supposed to do. Get into the habit of commenting on your scripts. Comment such that someone completely unfamiliar with your script could open it and know exactly what’s happening in every line. Do not fall into the classic trap of thinking “this is an easy step, I’ll totally remember what it’s doing” because it’s not and you won’t.\nDeveloping your own individual coding style is part of learning R, and will most likely help make it more intuitive and enjoyable. However, it will save you a lot of time and energy (and tears) if you make sure that your individual style is built on a foundation of good habits. Name data objects in consistent ways. Capitalize names in a consistent way within and across all your scripts. Use the same indentation strategies. Comment on everything. Developing smart coding habits early on is much easier than trying to break bad habits later.\nNote: Many analyses require coordination between multiple R scripts. It might also just make more sense to break a long script into multiple shorter scripts. Keeping collections of R scripts organized and functional is best accomplished using something called an R Project. We will cover these later, but you can find some information on their uses and structuring here and here if you’re interested."
  },
  {
    "objectID": "02a_Tutorial_1.html#variable-assignment",
    "href": "02a_Tutorial_1.html#variable-assignment",
    "title": "Part 1 - R Basics",
    "section": "Variable Assignment",
    "text": "Variable Assignment\nWe’ve already mentioned that assignment to variables stores information in R. This is true for any of the data structures we’ll talk about. You can assign information in two main ways, with ‘=’ or with ‘<-’. In general, these are equivalent methods of assignment. Try assigning a variable using both methods.\nNote that ‘x -> 1’ is not the same thing as ‘x <- 1’.\n\nx = 1\ny <- 2\nz -> 3\n## Error in 3 <- z: invalid (do_set) left-hand side to assignment\n\nprint(x)\n## [1] 1\n\nprint(y)\n## [1] 2\n\nprint(z)\n## Error in print(z): object 'z' not found\n\nVariables can be used to store more than just numbers though. Variables can store a wide-variety of data objects. This includes characters (or strings of characters). It’s important to use quotation marks when assigning characters to a variable (this prevents the assignment of previously assigned numerical values on accident). Remember, R cannot infer your intentions.\n\nbeta = x\nprint(beta)\n## [1] 1\n\nalpha = \"x\"\nprint(alpha)\n## [1] \"x\""
  },
  {
    "objectID": "02a_Tutorial_1.html#vectors",
    "href": "02a_Tutorial_1.html#vectors",
    "title": "Part 1 - R Basics",
    "section": "Vectors",
    "text": "Vectors\nVectors are ordered collections of either numbers or characters. These are one dimensional data structures, meaning their structure is defined by a length; every value occupies a specific position in the vector. Position 1 is the first value, position 2 is the second, etc. etc.\nVectors can be created manually using the concatenate function, c(). This works for both individual values, character strings, and even other vectors. This function essentially binds objects together in a specific order. You can mix-and-match values and variables when concatenating.\n\nx = c(1,2,3)\ny = c(4,5,6)\nprint(x)\n## [1] 1 2 3\n\n#Notice that we have now overwritten the original values assigned to x and y\n\nz = c(y,x)\nprint(z)\n## [1] 4 5 6 1 2 3\n\n#Notice that the order in which values were assigned depends on both\n#the order of the vectors and the order of values within the vectors\n    \nA = \"red fish\"\nB = \"blue fish\"\n\npond = c(A, B, \"one fish\", \"two fish\")\nprint(pond)\n## [1] \"red fish\"  \"blue fish\" \"one fish\"  \"two fish\"\n\nThe concatenate function is a simple way to create short vectors, but longer objects can be tedious to input in this manner. There are more efficient ways to create vectors if you’d like them to contain simple sequences or patterns of numbers.\nFor instance, you can create a vector with all integers between two numbers by putting a ‘:’ between them. Note that order matters.\n\nforward = c(1:10)\nreverse = c(10:1)\n\nprint(forward)\n##  [1]  1  2  3  4  5  6  7  8  9 10\nprint(reverse)\n##  [1] 10  9  8  7  6  5  4  3  2  1\n\nThis is the equivalent of: c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) or c(10, 9, 8, 7, 6, 5, 4, 3, 2, 1), but requires a fraction of the number of keystrokes.\n\nseq() and rep()\nThe functions seq() and rep() can generate more complex patterns. The seq() function can be used to define a sequence of values based on the desired start and stop values, as well as the interval between values. Note that if a different interval size is not specified, the function defaults to a value of 1, and this command produces the same thing as you’d get by using a ‘:’.\n\nsequence = seq(from = 3, to = 15)\nprint(sequence)\n##  [1]  3  4  5  6  7  8  9 10 11 12 13 14 15\n\nnew_sequence = seq(from = 0, to = 15, by = 3)\nprint(new_sequence)\n## [1]  0  3  6  9 12 15\n\nIf you’d like to build a vector based on the repetition of values, rep() can be used. The vector is formed based on an input (specified as ‘x’) and a desired number of times to repeat that input.\n\nrepetition = rep(x = 7, times = 8)\nprint(repetition)\n## [1] 7 7 7 7 7 7 7 7\n\nrep = rep(x = \"rep\", times = 10)\nprint(rep)\n##  [1] \"rep\" \"rep\" \"rep\" \"rep\" \"rep\" \"rep\" \"rep\" \"rep\" \"rep\" \"rep\"\n\nx_rep = rep(x = x, times = 4)\nprint(x_rep)\n##  [1] 1 2 3 1 2 3 1 2 3 1 2 3\n\nAnother useful parameter for the rep() function is “each”, which tells R how many times to repeat each value within the repetition. By manipulating ‘times’ and ‘each’, you can create vectors which would otherwise take a lot of time to assemble manually.\n\nnew_rep = rep(z, times = 4, each = 1)\nprint(new_rep)\n##  [1] 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3\n\nsingle_rep = rep(z, times = 1, each = 4)\nprint(single_rep)\n##  [1] 4 4 4 4 5 5 5 5 6 6 6 6 1 1 1 1 2 2 2 2 3 3 3 3\n\n\n\nVector Arithmetic\nOne of the other useful vector characteristics is that once you’ve generated a vector, it can easily be used in arithmetic and mathematical functions.\n\nprint(z)\n## [1] 4 5 6 1 2 3\n\nz + z \n## [1]  8 10 12  2  4  6\n\nNotice how the vectors were added together. Vector expressions are site specific: z1 + z1, z2 + z2, etc. However, if the vector lengths don’t match, the shorter vector gets recycled.\n\nm = c(1,2,1)\nz + m\n## [1] 5 7 7 2 4 4\n\nz + 6 #Essentially a vector of length 1\n## [1] 10 11 12  7  8  9\n\n\n\nSummarizing Vectors\nVectors are commonly used to store numerical measurements or sample data. It’s therefore useful to be able to summarize them in various ways. There are many functions to do this with, most of which are fairly intuitive. For example, asking for the max() of a vector will return the maximum value contained in the vector.\n\nprint(z)\n## [1] 4 5 6 1 2 3\n\nmax(z)\n## [1] 6\n\nOther common summary functions are min(), mean(), and median(). It is often also useful to find the length of a vector.\n\nprint(z)\n## [1] 4 5 6 1 2 3\nlength(z)\n## [1] 6\n\nprint(m)\n## [1] 1 2 1\nlength(m)\n## [1] 3\n\nOther functions output more complex results. The range() function returns two numbers at the same time, the maximum and the minimum values. This presents a new challenge. If you wanted to use these numbers for further analysis, you will need to be able to isolate them or specify which you want to use. This is where understanding the structure of the data object becomes important.\nAssign the range of ‘z’ to a variable. Print that variable. What do you see? Using str() will return a compact summary of the structure of an object. Use str() to examine both z and your new variable. Don’t forget that your variable name should be both concise and informative!\nBoth objects are vectors. Remember that because vectors are one-dimensional data objects (the structure is defined by a length), the values contained within them can be identified by their positions. Calling up specific values from a vector simply requires specifying the position of the value(s). This is done using ‘[ ]’, and is the first example of a process called subsetting. This is one of the most useful skills in R, especially once we start dealing with larger, multi-dimensional data structures.\n\nprint(z)\n## [1] 4 5 6 1 2 3\n\n#Print the second position in vector z\nprint(z[2])\n## [1] 5\n\n#Remember how we can specify sequences of numbers using ':'\nprint(z[2:4])\n## [1] 5 6 1\n\nHere’s your first multi-step problem: Assign just the maximum value of ‘z’ to a new variable by subsetting the range variable you previously created. Things to keep in mind: Are your variable names short but informative? Have you been commenting on your script? What would be a more efficient way of doing this?\nThe functions we’ve dealt with so far work with ordered (ex - vectors that are arranged from smallest to largest) and non-ordered vectors. However, some functions or manipulations require that vector inputs are ordered. This can be accomplished using the function sort(). This function will re-arrange values within a vector from smallest to largest. How could you use sort() and length() to find the maximum value of a vector? As you can see, for any analysis in R, there can be many potential solutions. For the simple question “what is the maximum value contained in vector z”, for example, we’ve already covered three possible solutions of varying complexity. The best solution will often be context-specific, but ultimately, if your solution works, it is a correct solution.\n\nprint(z)\n## [1] 4 5 6 1 2 3\n\nsort(z)\n## [1] 1 2 3 4 5 6\n\n\n\nChallenge: Can you use sort() to arrange a vector from largest to smallest instead?\nThere are several ways to do this. Be creative!"
  },
  {
    "objectID": "02a_Tutorial_1.html#data-classes",
    "href": "02a_Tutorial_1.html#data-classes",
    "title": "Part 1 - R Basics",
    "section": "Data Classes",
    "text": "Data Classes\nUp to this point we’ve mostly dealt with data comprised of numbers or characters. R handles several different types of data though, which is why it’s such a powerful tool.\n\nIntegers, Numerics, and Characters\nInteger and Numeric data are both comprised of number values. Integers cannot have decimals while numerics can.\nCharacter data is non-numeric. Letters or words are (usually) automatically stored as character data. Numbers can also be stored as characters too, but will lose their numeric properties.\n\nnum = c(14, 22, 30, 38)\nnum + num\n## [1] 28 44 60 76\n\nchar = c(\"14\", \"22\", \"30\", \"38\")\nchar + char\n## Error in char + char: non-numeric argument to binary operator\n\n\n\nFactors\nFactors identify discrete classifications (i.e. - categorical variables), and tells R it’s dealing with unique groups of data. The different groups of factors are called ‘levels’. These levels don’t have inherent meaning, other than to delineate data into groups. Factors can be identified by numbers or character data. Remember though, when a number is classified as a factor, it loses its numerical characteristics (you can’t add factors together, for example). We will talk about factors when we cover more complex analyses and data visualization, but common examples include experimental groups (control vs. treatment), descriptive characteristics (colour, population of origin, etc.), and other similar grouping information.\n\n\nLogical Data\nLogical data is a little different, and characterizes values based on whether they satisfy some logical statement. Logical data is therefore reported as True/False. There is a specific R syntax to define these statements. ‘==’ is used to indicate an exact equality is required. This is not the same as ‘=’. An exact equality is read as “is exactly equal to”. Looking for values that are not equal to a given value is accomplished with ‘!=’, the syntax for an inequality (AKA “is not equal to”). These logical statements can be evaluated for single values or for vectors. Pay particular attention to the output of using a logical evaluator on a vector.\n\nvalue = 4\n\n#Evaluates the statement \"value is exactly equal to 3\"\nvalue == 3\n## [1] FALSE\n\nprint(z)\n## [1] 4 5 6 1 2 3\nz == 4\n## [1]  TRUE FALSE FALSE FALSE FALSE FALSE\n\n#Evaluates the statement \"which positions in vector z are not equal to 3\"\nz != 3\n## [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n\nOther common logical statements are ‘<’ (“less than but not equal to”); ‘>’ (“greater than but not equal to”); ‘<=’ (“less than or equal to”); and ‘>=’ (“greater than or equal to”). Remember that’<-’ is used to assign values, whereas ‘<=’ is used to evaluate logical statements.\n\nprint(value)\n## [1] 4\n\nvalue > 3\n## [1] TRUE\n\nprint(z)\n## [1] 4 5 6 1 2 3\n\nz >= 2\n## [1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n\nEvaluating logical statements becomes increasingly important when dealing with large data sets. There are several other logical evaluators or modifiers that we will examine when we cover subsetting data in more detail.\nMissing values may be common in data sets, and constitute their own form of logical data. These can be represented in R using NA, which stands for ‘Not Available’.\nTRUE, FALSE, NA, and a few other values are exceptions to the rule that characters need to be input surrounded by quotation marks, since they’re interpretted as logical data, not character data. These characters are so fundamental to the operation of R that it recognizes them without quotation marks (remember this when assigning variable names!). In some cases, R also will recognize abbreviations of these values. Test this by assigning “T” and T to a variable and printing the variables.\n\n\nCombining Data Classes\nUsing the function class() is a simple method to determine what type of data an object contains. Check the type of data contained in ‘z’. What type of data is contained within the variable ‘pond’, which you assigned early on in this tutorial. One important thing to note, vectors can only contain one type of data. You cannot use a vector to store both numeric data and character data, for example. It will change all values to characters if you try to do this.\n\ncombo = c(z, pond)\n\nprint(combo)\n##  [1] \"4\"         \"5\"         \"6\"         \"1\"         \"2\"         \"3\"        \n##  [7] \"red fish\"  \"blue fish\" \"one fish\"  \"two fish\"\n\nclass(combo)\n## [1] \"character\"\n\nThe exception to this rule is when a vector contains ‘NA’ (and certain other logical values), in which case the vector retains the class of the non-missing values.\n\nmissing_z = z\n\n#Replaces the 4th position of missing_z with NA\nmissing_z[4] = NA\n\nprint(missing_z)\n## [1]  4  5  6 NA  2  3\n\nclass(missing_z)\n## [1] \"numeric\"\n\nThe exception to this exception is when TRUE and FALSE are included alongside numeric data. In that case, the logical values are converted to binary data (1 = TRUE, 0 = FALSE), rather than maintained as logical data.\n\nfalse_z = z\nprint(false_z)\n## [1] 4 5 6 1 2 3\n\nfalse_z[2] = FALSE\nprint(false_z)\n## [1] 4 0 6 1 2 3\n\nSometimes it’s necessary to modify the class of data values. This is known as coercion. The functions as.character(), as.numeric(), as.integer(), and as.factor() are commonly used to force data into character, numeric, integer, and factor classes, respectively.\n\nz.char = as.character(z)\nprint(z.char)\n## [1] \"4\" \"5\" \"6\" \"1\" \"2\" \"3\"\n\nz.int = as.integer(z.char)\nprint(z.int)\n## [1] 4 5 6 1 2 3"
  },
  {
    "objectID": "02a_Tutorial_1.html#creating-data-objects",
    "href": "02a_Tutorial_1.html#creating-data-objects",
    "title": "Part 1 - R Basics",
    "section": "Creating Data Objects",
    "text": "Creating Data Objects\nOne of the most critical requirements towards R literacy is the ability to generate data structures. Empty vectors can be created for numeric/integer and character data using numeric() and character(), respectively. An empty vector for data classified as logical is created with vector(). Empty vectors can be assigned any type of data though, which will often override the original data class. We will talk about an important exception below. Remember, objects you create are not stored unless they are assigned to a variable!\n\nN = numeric()\nC = character()\nV = vector()\n\nclass(N)\n## [1] \"numeric\"\nclass(C)\n## [1] \"character\"\nclass(V)\n## [1] \"logical\"\n\nLet’s see what happens when you try to assign a character to a position in a numeric vector.\n\nclass(N)\n## [1] \"numeric\"\nN[3] = \"character\" #this specifies that the third position in the \n                   #vector N is equal to the character string “character”. \nclass(N)\n## [1] \"character\"\n\nNotice how the original class for N has changed to match the class of what was added. If we add a numeric value to N now what will happen?\n\nN[6] = 15 #this specifies that the sixth position in the vector N is equal \n          #to the number 15. \n\nclass(N)\n## [1] \"character\"\n\nEven though we added a number to vector N, because it already contains a character string, and is a character-classed vector, R automatically converts the numeric input to a character. This illustrates the important exception mentioned before: Vectors classed as character data coerce any input to character data. This is a common issue when working with experimental data - units or unrecognized NA filler variables can often force data from numeric to character class. This is also applies to empty character vectors.\n\nclass(C)\n## [1] \"character\"\n\nC[3] = 4\nprint(C)\n## [1] NA  NA  \"4\"\n\nclass(C)\n## [1] \"character\"\n\nC[5] = TRUE\nprint(C)\n## [1] NA     NA     \"4\"    NA     \"TRUE\"\n\nclass(C)\n## [1] \"character\""
  },
  {
    "objectID": "02a_Tutorial_1.html#multi-dimensional-data-structures",
    "href": "02a_Tutorial_1.html#multi-dimensional-data-structures",
    "title": "Part 1 - R Basics",
    "section": "Multi-Dimensional Data Structures",
    "text": "Multi-Dimensional Data Structures\nLike vectors (one-dimensional structures defined by a length), the other common data structures can be defined by different numbers of dimensions. A matrix is a two-dimensional object (defined by a length and a width, aka some number of rows and columns). An array is an n-dimensional object. For example, a three-dimensional array is commonly thought of as a cube, defined by some number of rows and columns replicated across some number of sheets.\nWhen creating an matrix or array, you need to specify the dimensions. Matrices are formed using the function matrix(nrow = X, ncol = Y). Arrays follow a slightly different format: array(dim = c(rows, columns, sheets)).\n\nmat = matrix(nrow = 5, ncol = 3)\nprint(mat)\n##      [,1] [,2] [,3]\n## [1,]   NA   NA   NA\n## [2,]   NA   NA   NA\n## [3,]   NA   NA   NA\n## [4,]   NA   NA   NA\n## [5,]   NA   NA   NA\n\narr = array(dim = c(3,4,3))\nprint(arr)\n## , , 1\n## \n##      [,1] [,2] [,3] [,4]\n## [1,]   NA   NA   NA   NA\n## [2,]   NA   NA   NA   NA\n## [3,]   NA   NA   NA   NA\n## \n## , , 2\n## \n##      [,1] [,2] [,3] [,4]\n## [1,]   NA   NA   NA   NA\n## [2,]   NA   NA   NA   NA\n## [3,]   NA   NA   NA   NA\n## \n## , , 3\n## \n##      [,1] [,2] [,3] [,4]\n## [1,]   NA   NA   NA   NA\n## [2,]   NA   NA   NA   NA\n## [3,]   NA   NA   NA   NA\n\nData frames are matrix-like data structures, in that they’re two-dimensional. The key difference is that matrices are restricted to one class of data, while data frames can contain columns of different data classes. Data frames are therefore extremely useful as an intuitive way to record results of experiments and are the most common data structure we’ll work with. While a data frame can contain columns with different classes of data, all columns must be of the same length within the data frame, and data class can’t vary within a column (again, with the exceptions outlined above).\nMatrices can easily be converted into data frames using the as.data.frame() function.\n\nprint(mat)\n##      [,1] [,2] [,3]\n## [1,]   NA   NA   NA\n## [2,]   NA   NA   NA\n## [3,]   NA   NA   NA\n## [4,]   NA   NA   NA\n## [5,]   NA   NA   NA\n\ndat = as.data.frame(mat)\nprint(dat)\n##   V1 V2 V3\n## 1 NA NA NA\n## 2 NA NA NA\n## 3 NA NA NA\n## 4 NA NA NA\n## 5 NA NA NA\n\nNotice that the column names change as the structure is converted from matrix to data frame. The row numbers also change slightly in their format. There are other differences between matrices and data frames that we won’t cover yet."
  },
  {
    "objectID": "02a_Tutorial_1.html#subsetting-data-structures",
    "href": "02a_Tutorial_1.html#subsetting-data-structures",
    "title": "Part 1 - R Basics",
    "section": "Subsetting Data Structures",
    "text": "Subsetting Data Structures\nNow that we’ve covered common data structures, it’s time to re-visit subsetting. Data sets can be quite large in R, and being able to efficiently and accurately pull out particular records or observations is critical for many analyses. Since data sets are rarely sorted in a way to makes this easily accomplished manually, you will have to rely on subsetting. The syntax of subsetting can be tricky to wrap your head around, but just keep the structure of the data in mind.\n\nPositional Subsetting\nWe already worked with subsetting vectors. These one-dimensional structures are subset by specifying the specific position of values you want. Matrices and data frames can be subset in a similar way, but now you’re dealing with two-dimensional structures. You need to specify both a row and a column (in that order) to locate a specific value, again using [] to contain this position information.\n\n#This time we add a little data when we create the matrix\ndat_matrix = matrix(nrow = 8, ncol = 4, data = 1:(8*4))\nprint(dat_matrix)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    9   17   25\n## [2,]    2   10   18   26\n## [3,]    3   11   19   27\n## [4,]    4   12   20   28\n## [5,]    5   13   21   29\n## [6,]    6   14   22   30\n## [7,]    7   15   23   31\n## [8,]    8   16   24   32\n\ndat_matrix[7,3] #Pulls out the value in row 7, column 3\n## [1] 23\n\nPositional subsetting can also use more complex information to locate data. For example, let’s say you wanted every other value from the third column. There are several ways to do this, other than manually inputting the numeric position of each number.\n\nprint(dat_matrix)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    9   17   25\n## [2,]    2   10   18   26\n## [3,]    3   11   19   27\n## [4,]    4   12   20   28\n## [5,]    5   13   21   29\n## [6,]    6   14   22   30\n## [7,]    7   15   23   31\n## [8,]    8   16   24   32\n#Approach 1 - Using c()\nc_positions = c(1,3,5,7) #Manually input positions into a vector\nprint(c_positions)\n## [1] 1 3 5 7\n\ndat_matrix[c_positions, 3] #Use the vector to specify rows you want from column 3\n## [1] 17 19 21 23\n\n#Approach 2 - Using seq()\nseq_positions = seq(from = 1, to = 8, by = 2)\nprint(seq_positions)\n## [1] 1 3 5 7\n\ndat_matrix[seq_positions, 3]\n## [1] 17 19 21 23\n\n#Approach 3 - Using vector arithmetic\narith_positions = (2 * 1:(8/2)) - 1 \n#Starts off with a vector that is half as long as the column, \n#multiplies those numbers by two, then subtracts one\nprint(arith_positions)\n## [1] 1 3 5 7\n\ndat_matrix[arith_positions, 3]\n## [1] 17 19 21 23\n\nObviously, some approaches are more intuitive than others. But different situations may require different things, so it’s a good idea to keep an open mind.\nWhat if you want to call an entire row or column? Again there are several ways to do this, but R has provided a shortcut: Leave either the row or column value empty.\n\nprint(dat_matrix)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    9   17   25\n## [2,]    2   10   18   26\n## [3,]    3   11   19   27\n## [4,]    4   12   20   28\n## [5,]    5   13   21   29\n## [6,]    6   14   22   30\n## [7,]    7   15   23   31\n## [8,]    8   16   24   32\n\ndat_matrix[,3] #Specifying a column, but not a row calls up the entire column\n## [1] 17 18 19 20 21 22 23 24\n\ndat_matrix[2,] #Specifying a row, but not a column calls up the entire row\n## [1]  2 10 18 26\n\nPositional subsetting can also be used to exclude values from a vector or matrix. Exclusive subsetting return all values except those in specified positions. Observe what happens when you include a ‘-’ when subsetting based on the previously defined positions.\n\nprint(dat_matrix)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    9   17   25\n## [2,]    2   10   18   26\n## [3,]    3   11   19   27\n## [4,]    4   12   20   28\n## [5,]    5   13   21   29\n## [6,]    6   14   22   30\n## [7,]    7   15   23   31\n## [8,]    8   16   24   32\n\ndat_matrix[seq_positions, 3]\n## [1] 17 19 21 23\n\ndat_matrix[-seq_positions, 3]\n## [1] 18 20 22 24\n\nWhen you’re defining specific positions to exclude, parentheses are important: -1:5 is not the same as -(1:5).\n\nprint(dat_matrix)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    9   17   25\n## [2,]    2   10   18   26\n## [3,]    3   11   19   27\n## [4,]    4   12   20   28\n## [5,]    5   13   21   29\n## [6,]    6   14   22   30\n## [7,]    7   15   23   31\n## [8,]    8   16   24   32\n\ndat_matrix[-(1:5),1]\n## [1] 6 7 8\n\ndat_matrix[-1:5,1]\n## Error in dat_matrix[-1:5, 1]: only 0's may be mixed with negative subscripts\n\nNOTE: If you’re storing data after an exclusive subset, make sure you’re assigning it to a new variable, not overwriting the old variable. If you’re not careful, exclusive subsetting is an easy way to lose data.\n\n#If you use just re-assign to the same variable...\nexcl_matrix = dat_matrix\nprint(excl_matrix)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    9   17   25\n## [2,]    2   10   18   26\n## [3,]    3   11   19   27\n## [4,]    4   12   20   28\n## [5,]    5   13   21   29\n## [6,]    6   14   22   30\n## [7,]    7   15   23   31\n## [8,]    8   16   24   32\n\nexcl_matrix = excl_matrix[-(1:2),] #Removes the first two rows\nprint(excl_matrix)\n##      [,1] [,2] [,3] [,4]\n## [1,]    3   11   19   27\n## [2,]    4   12   20   28\n## [3,]    5   13   21   29\n## [4,]    6   14   22   30\n## [5,]    7   15   23   31\n## [6,]    8   16   24   32\n\n#If you accidentally run that line of code again, you'll lose ANOTHER two rows\nexcl_matrix = excl_matrix[-(1:2),] \nprint(excl_matrix)\n##      [,1] [,2] [,3] [,4]\n## [1,]    5   13   21   29\n## [2,]    6   14   22   30\n## [3,]    7   15   23   31\n## [4,]    8   16   24   32\n\n\n#If you use a new variable name...\nexcl_matrix = dat_matrix\nprint(excl_matrix)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    9   17   25\n## [2,]    2   10   18   26\n## [3,]    3   11   19   27\n## [4,]    4   12   20   28\n## [5,]    5   13   21   29\n## [6,]    6   14   22   30\n## [7,]    7   15   23   31\n## [8,]    8   16   24   32\n\nnext_matrix = excl_matrix[-(1:2),] #Removes the first row\nprint(next_matrix)\n##      [,1] [,2] [,3] [,4]\n## [1,]    3   11   19   27\n## [2,]    4   12   20   28\n## [3,]    5   13   21   29\n## [4,]    6   14   22   30\n## [5,]    7   15   23   31\n## [6,]    8   16   24   32\n\n#Now if you accidentally run the line, you've still got all the data you wanted\nnext_matrix = excl_matrix[-(1:2),]\nprint(next_matrix)\n##      [,1] [,2] [,3] [,4]\n## [1,]    3   11   19   27\n## [2,]    4   12   20   28\n## [3,]    5   13   21   29\n## [4,]    6   14   22   30\n## [5,]    7   15   23   31\n## [6,]    8   16   24   32\n\nIn many cases rather than removing data from specific positions, it’s usually a better idea to call data that fulfills some requirement. This also has the benefit of working when you don’t know the exact positions of wanted vs. unwanted data. This is called logical subsetting, since it’s based on the evaluation of logical statements. Buckle up, this is where the syntax can get a little weird.\n\n\nLogical Subsetting\nLogical subsetting will return values that result in a TRUE evaluation of the logical statement.\n\nprint(dat_matrix)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    9   17   25\n## [2,]    2   10   18   26\n## [3,]    3   11   19   27\n## [4,]    4   12   20   28\n## [5,]    5   13   21   29\n## [6,]    6   14   22   30\n## [7,]    7   15   23   31\n## [8,]    8   16   24   32\n\ndat_matrix[,1] > 3\n## [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\ndat_matrix[dat_matrix[,1] > 3, 1]\n## [1] 4 5 6 7 8\n\nLet’s break that syntax down. Remember, subsetting matrices requires identifying positions based on rows and columns. In the above expression, you’re using the logical statement ‘dat_matrix[,1] > 3’ instead of specifying row numbers: The positions in the first column that satisfy the logical statement are used as a substitute for row numbers. As your data sets get more complex, logical subsetting becomes more and more useful, as it allows you to subset without needing to know the exact positions of the desired data.\nCheck Point: What type of data structure is produced by the subsets we’ve shown so far?\nThe final utility of logical subsets we’ll cover for matrices is creating a new matrix by selecting entire rows or columns in an existing matrix. We’ve already talked about how to call an entire row or column in a matrix (just leave out the column or row position, respectively). We can combine this syntax with a logical subset to pull entire rows or columns that fulfill the logical statement.\n\nprint(dat_matrix)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    9   17   25\n## [2,]    2   10   18   26\n## [3,]    3   11   19   27\n## [4,]    4   12   20   28\n## [5,]    5   13   21   29\n## [6,]    6   14   22   30\n## [7,]    7   15   23   31\n## [8,]    8   16   24   32\n\ndat_matrix[,1] > 3\n## [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\ndat_matrix[dat_matrix[,1] > 3 ,] #Note: no column position specified\n##      [,1] [,2] [,3] [,4]\n## [1,]    4   12   20   28\n## [2,]    5   13   21   29\n## [3,]    6   14   22   30\n## [4,]    7   15   23   31\n## [5,]    8   16   24   32\n\n#You're not restricted to just evaluating the first column either\ndat_matrix[,3] <= 20\n## [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\ndat_matrix[dat_matrix[,3] <= 20 ,] \n##      [,1] [,2] [,3] [,4]\n## [1,]    1    9   17   25\n## [2,]    2   10   18   26\n## [3,]    3   11   19   27\n## [4,]    4   12   20   28\n\nAgain, let’s break down that syntax. The first logical subset pulls the entire row when the value in the column 1 position is greater than, but not equal to, 3. The second subset pulls any row where the value in the column 3 position is less than or equal to 20. Subsets like this can be confusing because it looks like you’re specifying rows to pull based on a column. Just keep the structure of the data object in mind. Sometimes it’s useful to remember that the length of a column is equal to the number of rows, and vice versa. Specifying a position in a column is the same as specifying a row number.\n\n\nSubsetting Dataframes\nData frames can be subset in all the same ways as matrices. There is an additional method of subsetting that is specific to data frames though that is extremely useful. Remember that when we converted from a matrix into a data frame the main difference was the data frame had column names, while the matrix did not. These column names can be used to subset by specifying dataframe$name.\n\nprint(dat_matrix)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    9   17   25\n## [2,]    2   10   18   26\n## [3,]    3   11   19   27\n## [4,]    4   12   20   28\n## [5,]    5   13   21   29\n## [6,]    6   14   22   30\n## [7,]    7   15   23   31\n## [8,]    8   16   24   32\n\nmatrix_df = as.data.frame(dat_matrix)\nprint(matrix_df) #Columns have been named V1 though V4\n##   V1 V2 V3 V4\n## 1  1  9 17 25\n## 2  2 10 18 26\n## 3  3 11 19 27\n## 4  4 12 20 28\n## 5  5 13 21 29\n## 6  6 14 22 30\n## 7  7 15 23 31\n## 8  8 16 24 32\n\nmatrix_df$V3\n## [1] 17 18 19 20 21 22 23 24\n\nsubset_df = matrix_df[matrix_df$V2 > 14,] #Calls any row where the V2 value \n                                          #is greater than 14\n\nOne other nice feature of working with data frames is that it’s easy to create new columns, just by specifying them as dataframe$new_column. If you specify a column that does not exist, R will create a new column with that name.\n\nmatrix_df\n##   V1 V2 V3 V4\n## 1  1  9 17 25\n## 2  2 10 18 26\n## 3  3 11 19 27\n## 4  4 12 20 28\n## 5  5 13 21 29\n## 6  6 14 22 30\n## 7  7 15 23 31\n## 8  8 16 24 32\n\nmatrix_df$colour = rep(c(\"Red\", \"Blue\"), times = 4)\nmatrix_df\n##   V1 V2 V3 V4 colour\n## 1  1  9 17 25    Red\n## 2  2 10 18 26   Blue\n## 3  3 11 19 27    Red\n## 4  4 12 20 28   Blue\n## 5  5 13 21 29    Red\n## 6  6 14 22 30   Blue\n## 7  7 15 23 31    Red\n## 8  8 16 24 32   Blue\n\nred_df = matrix_df[matrix_df$colour == \"Red\",]\nred_df\n##   V1 V2 V3 V4 colour\n## 1  1  9 17 25    Red\n## 3  3 11 19 27    Red\n## 5  5 13 21 29    Red\n## 7  7 15 23 31    Red\n\nYou can also subset based on multiple logical statements. This is possible in both matrices and data frames, but the use of column names tends to help keep the statements from getting to be too confusing. There are a couple of these types of modifiers to logical statements. The modifier ‘&’ is perhaps the most common, and specifies positions that fulfil condition 1 AND condition 2. We will cover other modifiers in later exercises.\n\nmatrix_df\n##   V1 V2 V3 V4 colour\n## 1  1  9 17 25    Red\n## 2  2 10 18 26   Blue\n## 3  3 11 19 27    Red\n## 4  4 12 20 28   Blue\n## 5  5 13 21 29    Red\n## 6  6 14 22 30   Blue\n## 7  7 15 23 31    Red\n## 8  8 16 24 32   Blue\n\n#Note that different logical evaluators are NOT separated by commas\ndouble_sub = matrix_df[matrix_df$colour == \"Blue\" & matrix_df$V2 > 12,]\n\ndouble_sub\n##   V1 V2 V3 V4 colour\n## 6  6 14 22 30   Blue\n## 8  8 16 24 32   Blue"
  },
  {
    "objectID": "02a_Tutorial_1.html#importing-data",
    "href": "02a_Tutorial_1.html#importing-data",
    "title": "Part 1 - R Basics",
    "section": "Importing data",
    "text": "Importing data\nMost of the time, data won’t be generated internally. Instead, files containing the data will need to be imported into the R environment. There are several tools to accomplish this. The easiest is to use the “Import Dataset” drop-down menu in the Environment window. This will allow you to select data sets in different formats to import. Pro tip: you can copy and paste the generated code sample into your script for easy access later!\nSometimes data sets need to be imported manually, either because additional details need to be specified or because the data format is not supported by the “Import Dataset” tool. This is especially common when working with more complex data (ex - satellite data or DNA sequences). There are several functions that can be used to import data in those formats, but because they are often highly specific to the type of data, we won’t cover them here.\nThe most important step before importing data is to set your working directory. This will tell R where to look for files. You can see what files R can “see” in the Files window (lower right quadrant). Remember, R cannot infer intentions; without the correct file path you cannot import data. You can set your working directory using the function setwd(“file_path”).\nTip: If you’re not exactly sure of the file path, it’s safest to use tab to navigate through your local file structure. Type ‘setwd(““)’, and then with your cursor placed inside the quotation marks, hit tab. R should provide a dropdown file tree that you can then use to navigate to the desired directory. In a later tutorial, we will cover the use of R Projects, which simplify this process.\nOnce you’ve imported the data, it’s always a good idea to check the structure and summary of the object. The summary() function will return a 6-number summary for each column.\nThis is the end of the first tutorial. Hopefully you’re beginning to feel more comfortable with the R interface, and the basics of data structuring and manipulation. The next section will deal with intermediate data manipulation. Later tutorials will focus on methods for summarizing data, common statistical analyses, and data visualization."
  },
  {
    "objectID": "01_git_and_R.html",
    "href": "01_git_and_R.html",
    "title": "Git, GitHub, R Projects, and RStudio",
    "section": "",
    "text": "Typically, we’ll work together to design a project from background justification to experimental design (always feel free to come to me with new project ideas at any stage though!). When setting up a project, we’ll follow a GitHub first, RProject second approach, meaning the first thing we’ll do is start the GitHub repo, and then we’ll bring that down onto your computer. I will set up a repo for the project on GitHub as part of the ZoopEcoEvo organization. The main benefits of this are that:\n\nThe project_template repo accessible as part of this organization includes a pre-determined directory structure, appropriate file paths, .Rmd YAMLs, etc. to help minimize how much set-up time is required from you.\nThis will keep all projects in a central location to make it easier to find relevant code and data.\nIt can act as a ‘directory’ of sorts; the list of members are great people to go to when you have questions or ideas!\n\n\n\nOnce the project repo is set up on GitHub, you need to get a copy of it downloaded onto your local machine (the computer you’ll be working on the code from). To do this:\n\n\n\n\n\n\n\n\nCopy the site path by clicking the green “< > Code” button in the top right corner of the repo structure.\nTypically, we’ll use HTTPS, so make sure this option is selected.\nThe link can be copied by clicking the overlapping squares icon to the right of the text.\nOpen RStudio and make a new project. Select version control (not new directory). In “Repository URL”, paste the URL for the GitHub repository that you copied above.\n\nMake sure that you keep your project folders organized on your local machine - I recommend having a single directory called “Lab_Projects” or something similar that you use to store all project folders.\n\n\nYou’ve now got a copy of the repo downloaded onto your local machine, which is linked to GitHub. We’ll discuss this more in other sections, but just to note it here: the main branch of every project is read only (i.e. - you likely won’t be able to directly modify the content). This is because the main branch represents the official “up to date” record of your project. To make changes, you’ll do all active development of the project on a separate branch and then merge this into the main branch. This adds a little complexity to the workflow, but means that you can freely explore any direction without worrying about losing progress you’ve already made; you can always just restore to the most recent stable version."
  },
  {
    "objectID": "01_git_and_R.html#making-a-working-branch",
    "href": "01_git_and_R.html#making-a-working-branch",
    "title": "Git, GitHub, R Projects, and RStudio",
    "section": "Making a working branch",
    "text": "Making a working branch\nRemember, the main branch of each project is read-only. In order to make changes, you’ll first have to make a new branch (purple connected-square icon in the top right corner of the Git pane of RStudio). Give this branch an informative name about the goal you’re working towards - exp_design, data_carpentry, linear_model, etc. This will help me keep track of the project status). You should see this branch appear on the GitHub repo as well. Merging these changes back into the main branch will be covered in the Project Development section."
  },
  {
    "objectID": "01_git_and_R.html#pull-commit-pull-push",
    "href": "01_git_and_R.html#pull-commit-pull-push",
    "title": "Git, GitHub, R Projects, and RStudio",
    "section": "Pull-commit-pull-push",
    "text": "Pull-commit-pull-push\nOnce your working branch is set up, you can begin populating the project with code and text. As you work on the development of the project, you can take snapshots of the code progress using “commits”, and store these snapshots on the virtual copy of your branch by “pushing” to GitHub. A good rule of thumb is to commit whenever you finish something you’d be annoyed to lose. Make sure your commit messages are concise but useful (see here for a nice approach).\nIt’s a good idea to “pull” the up-to-date code from the GitHub repo at the beginning of every session to keep the local and virtual copies of your working branch in sync. This is especially important if you are working from multiple machines.\nIf you’re the only one working on your code, you can focus on just your working branch. If other people are working on this project as well, however, you’ll have to make sure you keep your branch up-to-date with what’s happening on the main branch as well. If you don’t, you risk ending up with major conflicts when you go to merge your branch back into main.\n\nYou can pull from the main branch into your working branch by entering this into the terminal (not the console): git pull origin main"
  },
  {
    "objectID": "03c_reports.html",
    "href": "03c_reports.html",
    "title": "Project Reports",
    "section": "",
    "text": "Our workflow starts with plans for a project, and usually ends with a published manuscript. Between these two concrete ends points, however, is a (seemingly vast) stretch of project development. As you work through the intermediate steps, it’s helpful to have something to remind yourself of the current state of the analyses, quickly share results with collaborators (who might have a range of comfort levels looking over code), write grant progress reports, etc.\nThat’s where the project report file comes in. There’s a Reports directory within the Output section of the project folder. The .Rmd file found there should be used to generate the reports / summaries. This file is already formatted to do a couple useful things.\nFirst, when it’s knit (using the controller script) it will output both an HTML and a markdown file. The markdown file will display nicely on the GitHub repo, providing a nice virtual format to share updated results. The HTML file won’t display nicely on the GitHub, but it is an easier document to share with collaborators. In addition to some aesthetic elements that make it easier to navigate (e.g. the table of contents along the side rather than the top of the document), the HTML also includes code folding and some other helpful elements. Take the following figure, for example. You’ll see a small button labeled “Code” just above it. Clicking this will reveal the code used to generate the figure. This is particularly useful when sharing code with several collaborators. Chances are, not all collaborators will care to see a project report that’s been swamped by code; many would rather just see the figures and some concise text contextualizing and summarizing the results. The code folding allows you to satisfy these collaborators along with those people who might actually care to check over the code as well.\n\n\nCode\nlibrary(tidyverse)\n## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n## ✔ ggplot2 3.4.0      ✔ purrr   1.0.0 \n## ✔ tibble  3.1.8      ✔ dplyr   1.0.10\n## ✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n## ✔ readr   2.1.3      ✔ forcats 0.5.2 \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n\nmean_1 = 20\ndist_1 = rnorm(50, mean = mean_1, sd = 3)\n\nmean_2 = 25\ndist_2 = rnorm(50, mean = mean_2, sd = 3)\n\nsim_data = tibble(\"Condition A\" = dist_1, \n                  \"Condition B\" = dist_2) %>% \n  mutate(\"delta\" = `Condition B` - `Condition A`,\n         \"dist_from_mean\" = `Condition A` - mean_1)\n\n\nplot_a = sim_data %>% \n  mutate(\"id\" = row_number()) %>% \n  pivot_longer(cols = c(`Condition A`, `Condition B`), \n               values_to = \"Value\", \n               names_to = \"Condition\") %>% \n  mutate(\"Condition\" = fct_relevel(`Condition`, c(\"Condition A\", \"Condition B\"))) %>% \n  ggplot(aes(x = Condition, y = Value, group = id)) + \n  geom_line(linewidth = 0.5) + \n  geom_point(aes(fill = Condition), shape = 21, size = 3) + \n  scale_fill_manual(values = c(\"lightsteelblue3\", \"indianred\")) + \n  theme_bw(base_size = 16) + \n  theme(legend.position = \"none\",\n        panel.grid = element_blank())\n\nplot_b = ggplot(sim_data, aes(x = dist_from_mean, y = delta)) + \n  geom_smooth(method = \"lm\", colour = \"black\",\n              formula = 'y~x') + \n  geom_point(size = 2, shape = 21, stroke = 1) + \n  labs(x = \"Difference from Mean\",\n       y = \"Trait Change\") + \n  theme_bw(base_size = 16) + \n  theme(panel.grid = element_blank())\n\nggpubr::ggarrange(plot_a, plot_b,\n                  labels = c(\"A.\", \"B.\"))\n\n\n\n\n\n\n\n\n\nThe other useful thing the report.Rmd file will do is automatically export image files (in .pdf and .png formats) to the Output/Figures/ directory. This makes it easier to pull up-to-date figures for presentations, progress reports, and the manuscript draft. The last use is especially important - being able to include figures in the manuscript from files instead of having to run code each time you knit the document will save you a lot of time.\nYou should feel free to duplicate the report.Rmd file - making a separate report for different purposes, for example. This will allow extra flexibility with the content, format, and output. Another particularly useful ‘version’ of the markdown file is a digital lab notebook. You can use this modified version of the report to keep track of things you’ve tried. And since it’s an R markdown file, you can include the snippets of code that you’re working with. While Git/GitHub will provide robust version control, sometimes a visual log like this is really useful and can help you from going in circles.\nIf you do decide to create a digital lab notebook like this, however, be sure to use unique chunk labels when plotting figures (figure names are inherited from chunk labels), otherwise you’ll overwrite the figures from your other reports! You could also prevent the digital lab notebook from saving image files by changing the chunk options (see the code chunk below).\n\n\nCode\n#These are the options set at the beginning of the report.Rmd file\nknitr::opts_chunk$set(\n  echo = knitr::is_html_output(),\n  fig.align = \"center\",\n  fig.path = \"../Figures/markdown/\", #Remove this line and...\n  dev = c(\"png\", \"pdf\"), # ...this line to prevent the figures from being saved\n  message = FALSE,\n  warning = FALSE,\n  collapse = T\n)"
  },
  {
    "objectID": "02_coding.html",
    "href": "02_coding.html",
    "title": "Coding in R",
    "section": "",
    "text": "After the project is set up, your focus will shift to data collection and analysis. I expect that you perform your analyses in R. I don’t, however, expect you to have mastered R already. These are skills that you’ll develop over time - one of the best ways to learn is to analyze data you collect. You will need to have at least rudimentary R skills before you start, though, so I’ve put together some home brew tutorials on coding in R that cover what I see as the basics. These are just one set of resources out there that can help you learn R, brush up on what you already know, and build new skills. As discussed in one of the later tutorials, your coding style is mostly up to you. You should know, however, that I primarily work with the “tidyverse”, and recommend that you do too. There’s an excellent resource for learning to code in R using this approach here if you want to further explore this.\nAs you work through these tutorials, keep in mind that the goal is to provide an introduction to R and how it can be used to work with data and make graphics. This represents just the first step towards “fluency” (or even proficiency) in R. Consistent application of the material presented is required to maintain and improve your R skills.\nThere are two specific learning objectives:\n\nLearning Objective 1 - Achieve a level of understanding to be able to use Google to answer questions / overcome obstacles in R.\n\nBe able to identify your issue\nKnow what to look for / understand what you find on Google\n\n\nLearning Objective 2 - Be able to modify available R scripts to analyze and visualize data sets.\n\nUnderstand how data structures are imported and modified\nUnderstand the functions and syntax of available scripts"
  },
  {
    "objectID": "02f_code_style.html",
    "href": "02f_code_style.html",
    "title": "Coding Style",
    "section": "",
    "text": "Opening up an unfamiliar data set or script and attempting to understand what was done during an analysis can be a daunting task (almost as daunting as writing the script in the first place). As you work on your code, keep its “legibility” in mind - not only will this help other people understand your code, but it will help you understand the code if you return to a project and find you’ve forgotten what’s been done (and trust me, you will forget).\n\nStyle\nAs you code more, you will develop your own “coding style” (a series of decisions you make about alignment and spacing of code, naming of variables and functions, commenting strategies, script structuring, etc. with the goal of promoting code clarity and interpretability). Developing your own style is a good thing, as it makes coding feel more natural. I will generally not, therefore, dictate minor stylistic decisions; so long as your code is understandable, feel free to write however you’d like. That being said, there are two levels of consistency you need to consider:\n\nInternal consistency - Your code needs to be consistent within each project (and especially within each script). Changing up how variables are named, indentation strategies, etc. can make it more difficult to follow the overall flow of an analysis.\nGeneral coding style - At some point, you may find yourself working on several projects at the same time. Having a consistent coding style will make it easier to switch between projects. Keeping track of the small style details for each project quickly becomes frustrating, and can keep you from reaching your “flow state” (that state of mind where you are fully immersed in the problem you’re trying to solve). Sometimes it’s helpful to keep a “style guide” document where you can keep track of your naming conventions and other stylistic decisions until they become second nature.\n\nFor some in depth examples of what goes into a “coding style” see here and here.\n\n\nCommenting\nThere is a fine line between over- and under-commenting on code. How you choose to comment on your code is part of your personal coding style, but there are some best practices you should keep in mind (see here for some useful non-R-specific tips).\n\nComments should most often detail why something was done, and only rarely how it was done. Keep in mind that good comments do not excuse unclear code.\nSimilarly, comments shouldn’t duplicate the code. Not every line needs a comment associated with it detailing what it’s doing.\nIf you’re pulling code from elsewhere, cite it! This might be from a paper’s supplemental materials, a StackOverflow post, or a tutorial. In all cases, give clear attribution.\n\nRules are meant to be broken. These best practices are aimed at making code interpretable. Part of this is avoiding visual clutter. However, if you find it helps to comment more frequently as you start out, do that.\n\n\nREADME Files\nREADME files are crucial. These act as a roadmap for the project, which is especially important for the project folder setup we will generally use - keeping track of which scripts do what, and how they’re connected is going to be a big help for anyone trying to understand your analyses. These files require substantial effort, and should not be treated as an afterthought.\nThere are many different examples of how a README could be compiled and structured. The general format I recommend using is here (this is what is included in the project folder template). This reflects information, suggestions, and advice from the American Naturalist (here and here), Dryad (a popular data repository), and Cornell."
  },
  {
    "objectID": "02d_Tutorial_4.html",
    "href": "02d_Tutorial_4.html",
    "title": "Part 4 - TidyVerse",
    "section": "",
    "text": "THIS TUTORIAL STILL BEING DEVELOPED\n\nPrinciple behind TidyVerse (https://teachdatascience.com/tidyverse/)\n\n\nWorkflow\n # Tidy Data\n\n\nPipes\n\n\nMutate\n\n\nSummarize\n\n\nGrouping\n\n\nPivots"
  },
  {
    "objectID": "02b_Tutorial_2.html",
    "href": "02b_Tutorial_2.html",
    "title": "Part 2 - More Data Manipulation",
    "section": "",
    "text": "Hopefully you’re now familiar with the basic syntax of R and the structure of common data objects. As we saw in the first tutorial, both are important when manipulating data in what might seem like simple ways (e.g. - subsetting rows or columns in a dataframe). This section of the tutorial will further develop your ability to manipulate and analyze data using slightly more complicated code structures."
  },
  {
    "objectID": "02b_Tutorial_2.html#modifying-the-for-statement",
    "href": "02b_Tutorial_2.html#modifying-the-for-statement",
    "title": "Part 2 - More Data Manipulation",
    "section": "Modifying the for statement",
    "text": "Modifying the for statement\nFor statements are fairly flexible, despite (or maybe because of) their simplicity. You can use any variable name, for example. You can also modify the values the variable will take on - this component of the for statement is essentially just a vector of values; any vector can be used to determine the variable values the loop will cycle through.\n\nfor(variable in 3:7){\n  print(variable)\n}\n## [1] 3\n## [1] 4\n## [1] 5\n## [1] 6\n## [1] 7\n\n\nfor(a in seq(from = 3, to = 10, by = 2)){\n  print(a)\n}\n## [1] 3\n## [1] 5\n## [1] 7\n## [1] 9\n\n\nsentence = c(\"this\", \"vector\", \"contains\", \"text\")\nfor(word in sentence){\n  print(word)\n}\n## [1] \"this\"\n## [1] \"vector\"\n## [1] \"contains\"\n## [1] \"text\"\n\n\nThe counter\nIn some cases, you’ll want to keep track of how many iterations you’ve gone through, or the sum of values you’ve processed. To do this, we use a code object called a counter. Remember, each iteration is independent of the others - Any variables you assign inside the code chunk get reset at the beginning of the next iteration when the “for loop” runs the code chunk again. To keep track of the number of iterations or the sum of values though, you need to retain this information somehow. To get around this, you set the variable’s initial value before the “for loop”. Compare the output of the two examples below. Why are they producing different outputs?\n\nfor(i in 1:10){\n  a = 0\n  a = a + i\n  print(a)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n## [1] 6\n## [1] 7\n## [1] 8\n## [1] 9\n## [1] 10\n\n\nb = 0\nfor(i in 1:10){\n  b = b + i\n  print(b)\n}\n## [1] 1\n## [1] 3\n## [1] 6\n## [1] 10\n## [1] 15\n## [1] 21\n## [1] 28\n## [1] 36\n## [1] 45\n## [1] 55\n\nThe second example sums up all the values of i used in the “for loop”, using this counter object to “protect” the value from the iterative resets. Counters, as their name implies, are also used to count iterations. In the example below, the value of the counter goes up by one during each iteration. After the “for loop” has run through all of its iterations, printing the counter confirms that there were 10 iterations run.\n\ncounter = 0\nfor(i in 1:10){\n  counter = counter + 1\n}\n\nprint(counter)\n## [1] 10\n\n#note that in this specific instance the same \n#information can be retrieved by printing i\nprint(i)\n## [1] 10\n\nIn this simple case, you can get the same information by printing the variable ‘i’, but these counters can be useful as you get into more complex “for loop” structures. For example, what happens if we have a nested for loop? This refers to a “for loop” inside a “for loop”. For every individual iteration of the outside loop, the inside loop runs to completion.\n\ncounter = 0\nfor(i in 1:3){\n  for(j in 1:4){\n    print(c(i,j))\n    counter = counter + 1\n  }\n}\n## [1] 1 1\n## [1] 1 2\n## [1] 1 3\n## [1] 1 4\n## [1] 2 1\n## [1] 2 2\n## [1] 2 3\n## [1] 2 4\n## [1] 3 1\n## [1] 3 2\n## [1] 3 3\n## [1] 3 4\n\nprint(c(i, j, counter))\n## [1]  3  4 12\n\nIn this case, the number of iterations is different from the final values of either of the two variables. You can still simply take the product of the two final variable values to get the number of iterations, but in many cases, more complex “for loops” have maximum variable values that change across iterations, making this much more difficult. In those cases, the counter structure is the best option. Can you think of one of the weaknesses of a counter structure? What happens if you run the “for loop” multiple times without resetting the counter?\nCounters are also commonly used to count the occurrence of specific types or values of data (often referred to as a ‘flag’ instead of a counter in these cases). Again, oftentimes this data can be retrieved using something like table() or a logical subset, but as you get into more complex analyses and code, running a flag within or alongside another process can sometimes be useful.\n\ntabbies = 0\nfor(i in 1:length(cat_data$colour)){\n  if(cat_data$colour[i] == \"tabby\"){\n    tabbies = tabbies + 1\n  }\n}\n\nprint(tabbies)\n## [1] 2\n\nThe above example also introduces the final code structure we’ll be covering in this portion of the tutorial: the if / else statement."
  },
  {
    "objectID": "02e_Tutorial_5.html",
    "href": "02e_Tutorial_5.html",
    "title": "Part 5 - R Markdown",
    "section": "",
    "text": "One of the best parts of a project is being able to finally share the results of your analyses. At various points throughout the project, you’ll probably need to share different components of your analyses (code, visualizations, written reports, etc.) with different people. The traditional process may entail performing your analyses in R, exporting your figures and tables, and then manually inserting them into a word document with the written summary of your results. Alongside this written summary, you will then have to share the underlying code and raw data for those who are interested. For collaborators that don’t use R, you will likely have to provide written details on how you processed and analyzed the data, while collaborators who do use R might like to see more about the code you used to run the analyses. Helping to create streamlined summaries for a range of audiences, an essential component of our workflow is the R Markdown (.Rmd) file."
  },
  {
    "objectID": "02e_Tutorial_5.html#setting-up-your-r-markdown",
    "href": "02e_Tutorial_5.html#setting-up-your-r-markdown",
    "title": "Part 5 - R Markdown",
    "section": "Setting up your R Markdown",
    "text": "Setting up your R Markdown\nIn RStudio, a new R Markdown file can be created in the same way as a new script, using the page icon in the top left corner of the window. You’ll notice a number of options in the drop-down menu, which can be explored at your leisure. When you create a new Markdown file you’ll be presented with a series of fields and options. Title and Author can be filled in as appropriate. You can input a static date, or choose to use the current date each time you knit (i.e. - run the code for) the document. There are three primary outputs for a markdown file: an HTML, PDF, or Word document. Each come with their own advantages, some of which will be explored in this tutorial. Also, don’t be overly concerned about what you enter into these fields or which options you select - all of these can be changed using the first key component of the markdown file, the YAML."
  },
  {
    "objectID": "02e_Tutorial_5.html#yaml-yet-another-markup-language",
    "href": "02e_Tutorial_5.html#yaml-yet-another-markup-language",
    "title": "Part 5 - R Markdown",
    "section": "YAML (Yet Another Markup Language)",
    "text": "YAML (Yet Another Markup Language)\nThe YAML appears at the top of markdown files and will set key parameters of the document. Some key components of the YAML are explored below. After so much time working with R syntax, the YAML may appear strange, and almost human-readable. This is by design. However, while these sections are typically easier to read, they are just as sensitive to formatting (if not more so) than R code. If you edit the YAML, you should be careful to follow the indentation levels of the various components."
  },
  {
    "objectID": "02e_Tutorial_5.html#text-formatting",
    "href": "02e_Tutorial_5.html#text-formatting",
    "title": "Part 5 - R Markdown",
    "section": "Text Formatting",
    "text": "Text Formatting\nOne of the main reasons to use an R Markdown file is Markdown itself - simple text formatting that looks great. The basic components of a markdown doc are:\n\nPlain text. Entered just as you would with any word processor.\nHeadings. These are indicated using “#”. Different levels of headings are indicated by different numbers of “#” (Heading level 1 - #, heading level 2 - ##, heading level 3 - ###, etc.).\nBolded and italicized text. To bold text, put two asterisks on either end. To italicize, use just one asterisk.\nOther forms of text formatting and customizing the page layout are also possible, with a little more know-how."
  },
  {
    "objectID": "02e_Tutorial_5.html#code-chunks",
    "href": "02e_Tutorial_5.html#code-chunks",
    "title": "Part 5 - R Markdown",
    "section": "Code Chunks",
    "text": "Code Chunks\nThe other reason R Markdown documents are so powerful is the direct integration of R code.\nSTILL BEING DEVELOPED"
  },
  {
    "objectID": "02e_Tutorial_5.html#inline-r-code",
    "href": "02e_Tutorial_5.html#inline-r-code",
    "title": "Part 5 - R Markdown",
    "section": "Inline R Code",
    "text": "Inline R Code\nIn addition to the code chunks, you can run R code right in the main text of an R Markdown document. For example:\n\nThere are currently 21 documents in the directory used to create this website. Of those, 5 are markdown files.\n\nWhile this reads like normal text, if you were to look at the markdown file, you’d see this:\n\nBy including the R code directly in the text like that, the information on the total number of files and the number of R markdown files is kept up to date. This is a powerful tool when paired with the project folder setup. More than just keeping track of the number of files (although this can be useful too), inline code can present updated statistical results (p-values, correlation coefficients, etc.), or descriptions of the findings (which group has the largest trait value, which sampling site has the lowest latitude, etc.). You can’t completely automate the results section of a manuscript, but when used carefully, inline code can help you keep your reports and manuscripts up to date!"
  },
  {
    "objectID": "02e_Tutorial_5.html#pdf-outputs",
    "href": "02e_Tutorial_5.html#pdf-outputs",
    "title": "Part 5 - R Markdown",
    "section": "PDF Outputs",
    "text": "PDF Outputs\nSTILL BEING DEVELOPED"
  },
  {
    "objectID": "02e_Tutorial_5.html#word-documents",
    "href": "02e_Tutorial_5.html#word-documents",
    "title": "Part 5 - R Markdown",
    "section": "Word Documents",
    "text": "Word Documents\nSTILL BEING DEVELOPED"
  },
  {
    "objectID": "02e_Tutorial_5.html#html-outputs",
    "href": "02e_Tutorial_5.html#html-outputs",
    "title": "Part 5 - R Markdown",
    "section": "HTML Outputs",
    "text": "HTML Outputs\nAs you might have guessed from the format of these tutorial pages, I prefer to knit markdown files to HTML. I find that the formatting is cleaner, and the results are easier to share across systems. As we’ll see, there’s also a set of features that are unique to HTML outputs that help make these documents far easier to share with a broad audience (coders and non-coders alike), and help them navigate the document. Setting up these features is also incredibly easy, as you can see below (this is the YAML used to produce this document).\n\n\nCode Folding\nThe first of these features is Code Folding. You’ll find this specified in the first line of the html_document parameters. Below this chunk of text, you’ll notice a small button labelled “Code” and a simple plot. When you click this button, the code used to generate the plot will be revealed. Click it again, and the code folds back up into the unobtrusive button form. This feature is really useful when sending a report to collaborators with varying coding skill/interest levels: for collaborators that don’t care to read through the code used to analyze the data, big chunks of code are distracting and confusing; some collaborators care more about the code used than the written descriptions, however, and not providing any access to the code won’t help them as much. Code folding allows you to satisfy both parties.\n\n\nCode\nrandom_points = data.frame(\"id\" = c(1:100), \n                           \"value\" = rnorm(n = 1000, mean = 50, sd = 10)) %>%  \n  filter(value < 55)\n\nggplot(random_points, aes(x = value)) + \n  geom_histogram(binwidth = 1) + \n  theme_bw(base_size = 24) + \n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nTable of Contents\nAlong the right side of this document, you’ll find a table of contents. The second line of HTML parameters creates the table of contents (TOC), while the third line specifies that the TOC should ‘float’ along the left side of the document, no matter where you scroll in the document. The different sections and sub-sections of the TOC are determined by the heading levels used throughout the text (#, ##, ###, or ####). These headings are nested based on the order they are provided (e.g. - # Section 1, ## Sub-section 1a, ## Sub-section 1b, ### Sub-sub-section 1b-1, # Section 2, etc.)\n\n\nInteractive Tables & Figures\nA really powerful HTML feature is the inclusion of interactive tables. Now, rather than having to send collaborators a .csv file with the data alongside your summary, you can include the data in a compact, searchable, filterable table. Note, you might still have to send data in some cases, but this is a good way to facilitate exploration of the data set.\n\n\nCode\nlibrary(palmerpenguins)\nlibrary(DT)\n\ntable_data = palmerpenguins::penguins %>% \n  select(species, island, body_mass_g, sex, year)\n\ndatatable(table_data, rownames = FALSE, filter=\"top\", options = list(pageLength = 10, scrollX = T))"
  },
  {
    "objectID": "02c_Tutorial_3.html",
    "href": "02c_Tutorial_3.html",
    "title": "Part 3 - ggPlot and Data Visualization",
    "section": "",
    "text": "You probably have some intuitive sense that visualization is important for communicating information or data to an audience. I would argue that data visualization is not just important, but is one of the most important components of any data analysis project. Visualization is more than just making a graph, but should be considered as a fairly involved process of analyzing data, determining the best way to present it, and then building a concise, effective graphic. As we move through this tutorial, you might find that the most challenging skills you’ll be developing will not be the coding itself, but deciding how to use the code to turn data into effective visualizations.\nR is a popular coding language for data visualization. While base R does have a range of graphics commands, the real advantage comes from the myriad packages that have been developed for data visualization. These packages tend to improve on the capabilities of base R (on either the computational or the aesthetic fronts), and can be used to expand the accessibility of a wider range of data visualizations. One of the more prominent packages for data visualization is called ggPlot. There are several advantages to using ggPlot over other graphics packages, including the use of an intuitive syntax (once you learn how it works). We will also focus on ggPlot because it is an integral part of something called the “Tidyverse”, a collection of packages designed specifically for data science, that share an underlying design philosophy and code syntax.\nBecause ggPlot is such a popular package, there are many available resources that are both more extensive and more thorough than this single tutorial can be. Chief among these resources are the books “ggPlot2: Elegant Graphics for Data Analysis” by Hadley Wickham (who designed this package) and the “R Graphics Cookbook” by Winston Chang. Both are available for free here and here. These are fairly comprehensive resources and are sure to be useful to work through if you’re looking for a rigorous course on ggPlot. Keep in mind though that both R and the ggPlot package is an evolving language and there are already several instances in both books where code details are now out of date."
  },
  {
    "objectID": "02c_Tutorial_3.html#ggplot-components",
    "href": "02c_Tutorial_3.html#ggplot-components",
    "title": "Part 3 - ggPlot and Data Visualization",
    "section": "ggPlot Components",
    "text": "ggPlot Components\nYou essentially start with a blank canvas.\n\nggplot()\n\n\n\n\n\n\n\n\nA ggPlot graphic is built in layers from there. The data and a set of “global aesthetics” are generally defined in this first layer. Defining the data is fairly intuitive - you just need to tell ggPlot which dataframe (it should be a data frame, not a matrix) you’ll be visualizing data from. The aesthetics include things like which columns specify the x- and y-axes.\n\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm))\n\n\n\n\n\n\n\n\nNow our blank canvas includes labeled axes, with “body_mass_g” on the x-axis and “bill_length_mm” on the y-axis. The data and aesthetics you define in this initial step become the defaults for every other layer you define (which is why they’re often called “global” aesthetics). You can, however, override these and define new data or aesthetics for individual layers if needed.\nYou’ll notice, however, that there isn’t actually any data displayed yet. After defining the global aesthetics, you need to add the actual visual elements that show the data. These are called “geometric layers”, or geoms for short. These include things like points, lines, and boxes. Remember, these geoms inherit their aesthetic values and data as defined by your global aesthetics. We’ll add points to the graphic in the next plot.\n\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm)) + \n  geom_point()\n\n\n\n\n\n\n\n\nNow we can see the underlying data. Bill length is plotted against the body mass of the individual. It looks like bill length generally increases with body mass. However, these measurements were made for multiple species of penguins. You’d probably want to show this in the figure, so we’ll now add in another aesthetic, specifying that data for the different species should be shown in different colors.\n\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm, colour = species)) + \n  geom_point()\n\n\n\n\n\n\n\n\nData was collected across several years. Let’s plot the data again using different shapes for the different field seasons. We’ll also separate out the aesthetic arguments, one per line to make it a bit easier to keep track of them.\n\nggplot(penguins, aes(x = body_mass_g, \n                     y = bill_length_mm, \n                     colour = species, \n                     shape = year)) + \n  geom_point()\n## Error in `geom_point()`:\n## ! Problem while computing aesthetics.\n## ℹ Error occurred in the 1st layer.\n## Caused by error in `scale_f()`:\n## ! A continuous variable cannot be mapped to the shape aesthetic\n## ℹ choose a different aesthetic or use `scale_shape_binned()`\n\nOur first error! ggPlot is telling us that it’s having trouble assigning shapes to the year. Check the structure of the penguins dataset. What type of data is ‘year’? We can address this issue by coercing ‘year’ to a factor. This tells ggPlot that instead of interpreting ‘year’ as a number, it should treat it as a grouping ID. After you’ve plotted this data, check the structure of the penguins data again. Why didn’t ‘year’ change class?\n\nggplot(penguins, aes(x = body_mass_g, \n                     y = bill_length_mm, \n                     colour = species, \n                     shape = as.factor(year))) + \n  geom_point()\n\n\n\n\n\n\n\n\nNow that the data is successfully plotted, we have another issue. There’s starting to be too much going on in this plot, and it makes it difficult to pick out interesting patterns (if they exist). One way to cope with this is to “facet” your plot - to separate out the data into different panels for different. We’ll group the data by the year measurements were made.\n\nggplot(penguins, aes(x = body_mass_g, \n                     y = bill_length_mm, \n                     colour = species, \n                     shape = as.factor(year))) + \n  geom_point() +\n  facet_grid(as.factor(year)~.)\n\n\n\n\n\n\n\n\nggPlot graphics are often developed iteratively, adding and changing layers as you go until the graphic suits your needs and satisfies your individual sense of aesthetics. Essentially any feature on a ggPlot can be customized manually, but there are helpful “presets” that can make this process easier. For example, if we want to change the background from grey to white and increase the base font size, we can add a “theme_” layer.\n\nggplot(penguins, aes(x = body_mass_g, \n                     y = bill_length_mm, \n                     colour = species, \n                     shape = as.factor(year))) + \n  geom_point() +\n  facet_grid(as.factor(year)~.) +\n  theme_bw(base_size = 16)\n\n\n\n\n\n\n\n\nNow that the basic components of the plot are in place, we can start altering other aesthetics. For example, we’ll increase the size of the data points, change the x- and y-axis labels, and get rid of the grid lines in the background.\n\nggplot(penguins, aes(x = body_mass_g, \n                     y = bill_length_mm, \n                     colour = species, \n                     shape = as.factor(year))) + \n  geom_point(size = 2) +\n  facet_grid(as.factor(year)~.) +\n  xlab(\"Body Mass (g)\") + \n  ylab(\"Bill Length (mm)\") + \n  theme_bw(base_size = 16) + \n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nBy continuing to alter layers in ggPlot, you can fairly quickly arrive at near publication-ready figures. Remember, ggPlot builds the graphics from the back to the front - layers later in the code will be placed on top of the previous layers. For example, run the following code with the geom_smooth layer after the geom_point layer. How did it affect the plot?\n\nggplot(penguins, aes(x = body_mass_g, \n                     y = bill_length_mm, \n                     colour = species, \n                     shape = as.factor(year))) + \n  geom_smooth(method = \"lm\", se = T, size = 1) + \n  geom_point(size = 2) +\n  scale_colour_manual(values = c(\"forestgreen\", \"gold\", \"cornflowerblue\")) + \n  facet_grid(as.factor(year)~.) +\n  guides(shape = \"none\") + \n  xlab(\"Body Mass (g)\") + \n  ylab(\"Bill Length (mm)\") + \n  theme_bw(base_size = 16) + \n  theme(panel.grid = element_blank())\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nLet’s say, however, that we wanted to change which species is shown in green and which is shown in blue. We could attempt to change that by switching up the order of colors in the vector, but it’s going to be much faster to just specify which color should be assigned to which species. This can save a lot of time and frustration, especially when working with many groups. The easiest way to assign colors to groups is using a named vector.\n\nspecies_colors = c(\"Gentoo\" = \"forestgreen\", \"Chinstrap\" = \"gold\", \"Adelie\" = \"cornflowerblue\")\n               \nggplot(penguins, aes(x = body_mass_g, \n                     y = bill_length_mm, \n                     colour = species, \n                     shape = as.factor(year))) + \n  geom_smooth(method = \"lm\", se = T, size = 1) + \n  geom_point(size = 2) +\n  scale_colour_manual(values = species_colors) + \n  facet_grid(as.factor(year)~.) +\n  guides(shape = \"none\") + \n  xlab(\"Body Mass (g)\") + \n  ylab(\"Bill Length (mm)\") + \n  theme_bw(base_size = 16) + \n  theme(panel.grid = element_blank())\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nThese aesthetic decisions may seem frivolous, but can strongly affect the interpretation of your data by the audience. Ask yourself, are you more likely to think critically about data if it’s presented in the above figure, or the below figure?\n\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm, colour = species, shape = as.factor(year), group = species)) + \n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", colour = \"coral\") + \n  scale_colour_manual(values = c(\"green\", \"yellow\", \"skyblue\")) + \n  xlab(\"Mass\") + \n  ylab(\"Length\") + \n  theme_bw(base_size = 16) + \n  theme(panel.background = element_rect(fill = \"blue\"),\n        panel.grid = element_line(colour = \"red\"))\n## `geom_smooth()` using formula = 'y ~ x'\n## Warning: The following aesthetics were dropped during statistical transformation: shape\n## ℹ This can happen when ggplot fails to infer the correct grouping structure in\n##   the data.\n## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n##   variable into a factor?\n\n\n\n\n\n\n\n\nRemember that the process of data visualization is separate from, but not independent of formal statistical analysis. In this example, we may be interested in how the relationship between bill length and body mass differs between species and across years. In many cases, figures are useful visual aids to more rigorous statistical tests. We will cover some of these analyses in a later tutorial.\nOne of the advantages of ggPlot is that this same grammar can be used to create a wide range of graphics. Several different types are illustrated below.\n\nggplot(penguins, aes(x = body_mass_g, fill = species)) +\n  geom_histogram(colour = \"black\", size = 0.3, binwidth = 100) + \n  scale_fill_manual(values = c(\"forestgreen\", \"gold\", \"cornflowerblue\")) + \n  facet_grid(species~.) +\n  ylab(\"Frequency\") + \n  xlab(\"Body Mass (g)\") +\n  guides(fill = \"none\") + \n  theme_bw(base_size = 16) + \n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = sex)) +\n  geom_boxplot()  +\n  xlab(\"Species\") + \n  ylab(\"Body Mass (g)\") +\n  theme_bw(base_size = 16) + \n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nKeep in mind, this is only the briefest introduction to ggPlot. However, the two books referenced at the beginning contain information about most of the other geoms and aesthetics, useful tips and tricks, and other information you’d need to create any data visualization you would need. The R Graphics Cookbook is especially useful when you’re starting out as it provides code “recipes” for different types of ggPlots. If you run into issues, Google is your best friend. Because ggPlot is one of the most popular graphics packages, there are innumerous discussion boards, question and answer forums, and other resources that you can access. It’s almost guaranteed someone else has had the same question as you, asked it online, and had it answered in a dozen different ways. Another interesting resource is a project called “TidyTuesday”, during which people practice their data manipulation, analysis, and visualization skills. Intended primarily to help people practice their skills with the TidyVerse (covered in a later tutorial), the end product is typically a graphic created with ggPlot. These end products, and their associated code, are often shared via Twitter and GitHub."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro to the Work Flow",
    "section": "",
    "text": "In general, operating in academia means most projects will be geared towards eventual publication. Our goal is not just to share results, however, but to help people understand the analyses we did. All publications from our group will therefore include some way of accessing the associated data and scripts. However, this gets us only partway towards our goal of replicable and understandable analyses. Project materials need to be well organized and thoroughly documented (not just “accessible”) in order for other people to follow what was done. This will also help other lab members, if at some point someone wants to follow up on your project, expand the scope of data you collected, or apply similar analyses to their data sets. This goal of making your work accessible, replicable, and understandable shapes the workflow I will ask you to use.\n\nProject Folders\nThis workflow is organized around a “project folder” - a collection of all materials (background information, data, scripts, outputs, etc.) used for a project, contained within a single directory. This concept is described in these slides and here. I’ve set up a template project folder that we’ll generally use to get your work started. A map of this project folder is included below to give you some idea of the framework the workflow described on this page works within. How the project folder functions during Project Development is expanded in that section.\n\nAs you can probably already tell, the workflow also relies on Git and GitHub to version control your work, to collaborate and review code, and to share data and results. Project folders and GitHub integrate nicely, and we’ll use a specific workflow while developing projects and writing code based on “GitHub flow”.\n\nSee here for the GitHub description of this approach and here for more details on how it’s applied in a research lab setting.\n\n\n\nInitial Setup\nThis workflow tends to “just work” once everything is set up and you get familiar with it. However, there’s a fair amount of stuff to work out before you get to that point. Luckily, these are generally one-time investments (per machine, that is).\n\nYou should already be added to the GitHub organization as a member, but if not, remind me.\nInstall R and RStudio.\nMake sure you have git installed on your local machine by this point. There are some helpful instructions here if needed.\nMake sure you’ve got the connections between Git, GitHub, and RStudio worked out. See here for a nice walk-through. We will generally use HTTPS when working with GitHub in RStudio.\nIt’s a good idea to have a basic familiarity with the terminology of Git and GitHub before you really dive in. For example, you should know what “pull”, “commit”, “push”, and “branch” mean in the context of a GitHub repo. See:\n\nHere for the GitHub documentation, including separate pages for commits, pulling, and pushing. Note that while these pages assume you’ll be working with Git in the command line, rather than RStudio, the general descriptions still apply.\nHere for a very in-depth written intro.\nHere for a quick video introduction (starts at ~1:20 where the description of Git starts).\nHere for a nice visual representation of some of the basis Git/GitHub actions.\nHere for a walk-through of a fairly similar Git/GitHub/RStudio workflow. There’s some differences (or things you won’t have to worry about because I will take care of it for you when we set your project up), but the sections on ‘Creating an RStudio project from version control’ and ‘Working with Git from within RStudio’ are definitely relevant.\n\n\n\n\nGeneral Note\nIn addition to the data and code you work on, you can make valuable contributions to the development of the lab as a whole by writing up the “invisible” work that you did - what did it take to get to a point where you could begin to engage with the project work flow? Did you find any useful resources that helped you? Are there aspects that were confusing or poorly explained to you? Were there obstacles that could be removed? After working with these systems for a while, it’s difficult to accurately remember what it’s really like to be faced with these challenges for the first time - your insights here can help design as smooth an on-boarding experience as possible!\n\nSee here for the initial inspiration for this."
  }
]